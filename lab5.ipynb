{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(20, 15))\n",
    "# sns.scatterplot(x='x1', y='x2', hue='y', data=df)\n",
    "# plt.savefig('lab5scatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='x1', y='x2', hue='y', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def relu(z):\n",
    "#     return np.maximum(0, z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# def sigmoid_derivative(z):\n",
    "#     return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    s = np.multiply(y_true, np.log(y_pred)) + np.multiply((1 - y_true), 1 - np.log(y_pred))\n",
    "    return - 1 / y_true.shape[0] * (np.sum(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    \n",
    "    def __init__(self, layers=(2, 9, 1), lr=0.1, max_iter=100000, tol=0.001):\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tol  = tol\n",
    "        self.loss_history = []\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.arr = None\n",
    "        self.multiplier = None\n",
    "    \n",
    "    def init_weights(self, input_size):\n",
    "        np.random.seed(42)\n",
    "#         self.layers.insert(0, input_size)\n",
    "#         self.w = [np.random.normal(0, 0.2, (self.layers[i], self.layers[i + 1])) for i in range(len(self.layers) - 1)]\n",
    "        self.b = [np.zeros((self.layers[i], )) for i in range(1, len(self.layers))]\n",
    "        self.w = [2*np.random.random((self.layers[i], self.layers[i + 1])) - 1 for i in range(len(self.layers) - 1)]\n",
    "#         self.b = [np.random.randn(self.layers[i], ) for i in range(1, len(self.layers))]\n",
    "    \n",
    "    def forward_propagation(self, xx=None, predict=False):\n",
    "        x = xx if xx is not None else self.x\n",
    "        self.arr = [np.copy(x)]\n",
    "        Z = self.arr[0]\n",
    "        for i in range(len(self.layers) - 2):\n",
    "            Z = np.dot(Z, self.w[i])\n",
    "            self.arr.append(Z)\n",
    "            Z = sigmoid(Z)\n",
    "            self.arr.append(Z)\n",
    "#         L1 = np.dot(x, self.w[0])\n",
    "#         A1 = sigmoid(L1)\n",
    "#         L2 = np.dot(A1, self.w[1])\n",
    "#         A2 = sigmoid(L2)\n",
    "#         L3 = np.dot(A2, self.w[2])\n",
    "#         A3 = sigmoid(L3)\n",
    "#         self.arr.append(L1)\n",
    "#         self.arr.append(A1)\n",
    "#         self.arr.append(L2)\n",
    "#         self.arr.append(A2)\n",
    "#         self.arr.append(L3)\n",
    "#         self.arr.append(A2)\n",
    "        Z = np.dot(Z, self.w[-1])\n",
    "        self.arr.append(Z)\n",
    "        Z = sigmoid(Z)\n",
    "        if not predict:\n",
    "#             self.arr.append(A3)\n",
    "            self.arr.append(Z)\n",
    "            self.loss_history.append(loss(self.y, Z))\n",
    "        return Z\n",
    "    \n",
    "    def back_propagation(self, y_pred):\n",
    "        delta = self.y - y_pred\n",
    "        w_upd = []\n",
    "        w_ = self.w + [np.array(1)]\n",
    "        for i in range(len(self.layers) - 1, 0, -1):\n",
    "            delta = delta.dot(w_[i].T) * sigmoid_derivative(self.arr[2 * i])\n",
    "            dw = self.arr[2 * (i - 1)].T.dot(delta)\n",
    "            db = np.sum(delta, axis=0)\n",
    "            w_upd.append(dw)\n",
    "            w_upd.append(db)\n",
    "        for i in range(len(self.w) - 1, -1, -1):\n",
    "            self.w[-i - 1] += self.multiplier * w_upd[2 * i]\n",
    "#             self.b[-i - 1] += self.multiplier * w_upd[2 * i + 1]\n",
    "        self.arr = None\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.multiplier = self.lr / X.shape[0]\n",
    "        self.init_weights(x.shape[1])\n",
    "        for i in range(self.max_iter):\n",
    "#             print(f'epoch {i}')\n",
    "            y_pred = self.forward_propagation()\n",
    "            self.back_propagation(y_pred)\n",
    "    \n",
    "    def predict(self, x):\n",
    "#         return self.forward_propagation(x, predict=True)\n",
    "        return np.round(self.forward_propagation(x, predict=True))\n",
    "\n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        Plots the loss curve\n",
    "        '''\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"logloss\")\n",
    "        plt.title(\"Loss curve for training\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "X = Normalizer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :2]\n",
    "y = data[:, 2].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.43937186487826413,\n",
       " -0.4393711833676936,\n",
       " -0.4393705018642999,\n",
       " -0.43936982036808303,\n",
       " -0.43936913887904316,\n",
       " -0.43936845739718017,\n",
       " -0.4393677759224941,\n",
       " -0.4393670944549852,\n",
       " -0.43936641299465334,\n",
       " -0.43936573154149844,\n",
       " -0.4393650500955208,\n",
       " -0.43936436865672024,\n",
       " -0.43936368722509694,\n",
       " -0.4393630058006509,\n",
       " -0.43936232438338213,\n",
       " -0.43936164297329067,\n",
       " -0.4393609615703765,\n",
       " -0.4393602801746398,\n",
       " -0.43935959878608055,\n",
       " -0.4393589174046988,\n",
       " -0.43935823603049434,\n",
       " -0.43935755466346754,\n",
       " -0.4393568733036184,\n",
       " -0.43935619195094683,\n",
       " -0.43935551060545286,\n",
       " -0.4393548292671367,\n",
       " -0.4393541479359982,\n",
       " -0.4393534666120375,\n",
       " -0.4393527852952546,\n",
       " -0.4393521039856495,\n",
       " -0.4393514226832223,\n",
       " -0.43935074138797303,\n",
       " -0.43935006009990163,\n",
       " -0.4393493788190083,\n",
       " -0.43934869754529304,\n",
       " -0.43934801627875575,\n",
       " -0.43934733501939666,\n",
       " -0.4393466537672157,\n",
       " -0.43934597252221286,\n",
       " -0.4393452912843882,\n",
       " -0.43934461005374187,\n",
       " -0.4393439288302739,\n",
       " -0.4393432476139841,\n",
       " -0.43934256640487274,\n",
       " -0.4393418852029398,\n",
       " -0.4393412040081853,\n",
       " -0.4393405228206092,\n",
       " -0.43933984164021167,\n",
       " -0.4393391604669927,\n",
       " -0.43933847930095227,\n",
       " -0.43933779814209045,\n",
       " -0.43933711699040734,\n",
       " -0.43933643584590293,\n",
       " -0.4393357547085773,\n",
       " -0.43933507357843027,\n",
       " -0.4393343924554622,\n",
       " -0.43933371133967297,\n",
       " -0.4393330302310626,\n",
       " -0.4393323491296311,\n",
       " -0.4393316680353786,\n",
       " -0.4393309869483051,\n",
       " -0.4393303058684106,\n",
       " -0.4393296247956952,\n",
       " -0.4393289437301589,\n",
       " -0.43932826267180175,\n",
       " -0.4393275816206237,\n",
       " -0.43932690057662493,\n",
       " -0.4393262195398054,\n",
       " -0.4393255385101652,\n",
       " -0.4393248574877043,\n",
       " -0.4393241764724227,\n",
       " -0.43932349546432065,\n",
       " -0.43932281446339794,\n",
       " -0.43932213346965476,\n",
       " -0.439321452483091,\n",
       " -0.43932077150370685,\n",
       " -0.4393200905315022,\n",
       " -0.4393194095664773,\n",
       " -0.4393187286086321,\n",
       " -0.43931804765796634,\n",
       " -0.4393173667144805,\n",
       " -0.4393166857781743,\n",
       " -0.4393160048490481,\n",
       " -0.43931532392710165,\n",
       " -0.439314643012335,\n",
       " -0.4393139621047484,\n",
       " -0.4393132812043417,\n",
       " -0.43931260031111496,\n",
       " -0.43931191942506825,\n",
       " -0.43931123854620163,\n",
       " -0.43931055767451505,\n",
       " -0.4393098768100087,\n",
       " -0.4393091959526824,\n",
       " -0.43930851510253643,\n",
       " -0.4393078342595707,\n",
       " -0.4393071534237853,\n",
       " -0.4393064725951801,\n",
       " -0.4393057917737553,\n",
       " -0.4393051109595109,\n",
       " -0.43930443015244697,\n",
       " -0.4393037493525634,\n",
       " -0.43930306855986034,\n",
       " -0.43930238777433783,\n",
       " -0.43930170699599597,\n",
       " -0.4393010262248347,\n",
       " -0.439300345460854,\n",
       " -0.439299664704054,\n",
       " -0.43929898395443484,\n",
       " -0.43929830321199637,\n",
       " -0.43929762247673865,\n",
       " -0.43929694174866185,\n",
       " -0.4392962610277658,\n",
       " -0.43929558031405086,\n",
       " -0.43929489960751666,\n",
       " -0.43929421890816356,\n",
       " -0.4392935382159914,\n",
       " -0.4392928575310003,\n",
       " -0.43929217685319033,\n",
       " -0.43929149618256136,\n",
       " -0.4392908155191136,\n",
       " -0.4392901348628471,\n",
       " -0.43928945421376175,\n",
       " -0.4392887735718578,\n",
       " -0.4392880929371351,\n",
       " -0.43928741230959373,\n",
       " -0.43928673168923377,\n",
       " -0.43928605107605506,\n",
       " -0.439285370470058,\n",
       " -0.43928468987124236,\n",
       " -0.4392840092796082,\n",
       " -0.43928332869515563,\n",
       " -0.4392826481178847,\n",
       " -0.4392819675477954,\n",
       " -0.43928128698488766,\n",
       " -0.43928060642916167,\n",
       " -0.4392799258806175,\n",
       " -0.4392792453392551,\n",
       " -0.4392785648050745,\n",
       " -0.43927788427807574,\n",
       " -0.4392772037582588,\n",
       " -0.43927652324562383,\n",
       " -0.43927584274017073,\n",
       " -0.4392751622418998,\n",
       " -0.43927448175081074,\n",
       " -0.43927380126690385,\n",
       " -0.439273120790179,\n",
       " -0.4392724403206362,\n",
       " -0.43927175985827577,\n",
       " -0.43927107940309745,\n",
       " -0.4392703989551014,\n",
       " -0.4392697185142877,\n",
       " -0.4392690380806562,\n",
       " -0.43926835765420724,\n",
       " -0.4392676772349405,\n",
       " -0.43926699682285614,\n",
       " -0.43926631641795444,\n",
       " -0.43926563602023505,\n",
       " -0.4392649556296982,\n",
       " -0.43926427524634415,\n",
       " -0.43926359487017247,\n",
       " -0.4392629145011836,\n",
       " -0.4392622341393773,\n",
       " -0.4392615537847538,\n",
       " -0.439260873437313,\n",
       " -0.43926019309705494,\n",
       " -0.4392595127639798,\n",
       " -0.4392588324380875,\n",
       " -0.4392581521193779,\n",
       " -0.4392574718078515,\n",
       " -0.43925679150350794,\n",
       " -0.4392561112063475,\n",
       " -0.43925543091637,\n",
       " -0.4392547506335755,\n",
       " -0.4392540703579643,\n",
       " -0.43925339008953607,\n",
       " -0.4392527098282911,\n",
       " -0.4392520295742294,\n",
       " -0.4392513493273508,\n",
       " -0.4392506690876557,\n",
       " -0.43924998885514377,\n",
       " -0.43924930862981526,\n",
       " -0.43924862841167023,\n",
       " -0.43924794820070856,\n",
       " -0.4392472679969303,\n",
       " -0.43924658780033554,\n",
       " -0.4392459076109244,\n",
       " -0.4392452274286968,\n",
       " -0.43924454725365275,\n",
       " -0.43924386708579244,\n",
       " -0.43924318692511577,\n",
       " -0.43924250677162296,\n",
       " -0.4392418266253137,\n",
       " -0.4392411464861884,\n",
       " -0.43924046635424685,\n",
       " -0.439239786229489,\n",
       " -0.43923910611191536,\n",
       " -0.4392384260015254,\n",
       " -0.4392377458983195,\n",
       " -0.4392370658022977,\n",
       " -0.43923638571345985,\n",
       " -0.43923570563180614,\n",
       " -0.4392350255573364,\n",
       " -0.43923434549005086,\n",
       " -0.43923366542994957,\n",
       " -0.4392329853770324,\n",
       " -0.43923230533129964,\n",
       " -0.4392316252927511,\n",
       " -0.43923094526138684,\n",
       " -0.439230265237207,\n",
       " -0.43922958522021144,\n",
       " -0.43922890521040037,\n",
       " -0.4392282252077739,\n",
       " -0.43922754521233176,\n",
       " -0.4392268652240742,\n",
       " -0.4392261852430013,\n",
       " -0.43922550526911286,\n",
       " -0.4392248253024093,\n",
       " -0.43922414534289017,\n",
       " -0.43922346539055585,\n",
       " -0.43922278544540627,\n",
       " -0.43922210550744145,\n",
       " -0.4392214255766616,\n",
       " -0.4392207456530664,\n",
       " -0.43922006573665634,\n",
       " -0.43921938582743103,\n",
       " -0.43921870592539064,\n",
       " -0.43921802603053534,\n",
       " -0.4392173461428651,\n",
       " -0.4392166662623799,\n",
       " -0.43921598638907977,\n",
       " -0.4392153065229648,\n",
       " -0.43921462666403505,\n",
       " -0.4392139468122904,\n",
       " -0.43921326696773105,\n",
       " -0.43921258713035705,\n",
       " -0.43921190730016824,\n",
       " -0.43921122747716496,\n",
       " -0.43921054766134693,\n",
       " -0.4392098678527144,\n",
       " -0.4392091880512672,\n",
       " -0.43920850825700564,\n",
       " -0.4392078284699295,\n",
       " -0.439207148690039,\n",
       " -0.43920646891733406,\n",
       " -0.4392057891518147,\n",
       " -0.43920510939348106,\n",
       " -0.43920442964233314,\n",
       " -0.4392037498983709,\n",
       " -0.4392030701615945,\n",
       " -0.43920239043200393,\n",
       " -0.4392017107095992,\n",
       " -0.43920103099438024,\n",
       " -0.4392003512863473,\n",
       " -0.4391996715855002,\n",
       " -0.43919899189183925,\n",
       " -0.43919831220536426,\n",
       " -0.43919763252607524,\n",
       " -0.4391969528539723,\n",
       " -0.4391962731890556,\n",
       " -0.439195593531325,\n",
       " -0.4391949138807807,\n",
       " -0.4391942342374225,\n",
       " -0.4391935546012505,\n",
       " -0.43919287497226495,\n",
       " -0.4391921953504656,\n",
       " -0.43919151573585286,\n",
       " -0.4391908361284263,\n",
       " -0.43919015652818627,\n",
       " -0.43918947693513266,\n",
       " -0.4391887973492656,\n",
       " -0.4391881177705851,\n",
       " -0.4391874381990911,\n",
       " -0.43918675863478374,\n",
       " -0.4391860790776631,\n",
       " -0.43918539952772917,\n",
       " -0.4391847199849819,\n",
       " -0.43918404044942133,\n",
       " -0.4391833609210476,\n",
       " -0.43918268139986066,\n",
       " -0.43918200188586065,\n",
       " -0.4391813223790475,\n",
       " -0.4391806428794212,\n",
       " -0.43917996338698195,\n",
       " -0.4391792839017297,\n",
       " -0.4391786044236644,\n",
       " -0.43917792495278624,\n",
       " -0.4391772454890952,\n",
       " -0.43917656603259125,\n",
       " -0.4391758865832745,\n",
       " -0.439175207141145,\n",
       " -0.4391745277062027,\n",
       " -0.4391738482784475,\n",
       " -0.4391731688578799,\n",
       " -0.4391724894444995,\n",
       " -0.4391718100383064,\n",
       " -0.4391711306393008,\n",
       " -0.43917045124748266,\n",
       " -0.439169771862852,\n",
       " -0.4391690924854089,\n",
       " -0.4391684131151533,\n",
       " -0.43916773375208534,\n",
       " -0.43916705439620496,\n",
       " -0.4391663750475122,\n",
       " -0.4391656957060072,\n",
       " -0.4391650163716899,\n",
       " -0.43916433704456026,\n",
       " -0.43916365772461863,\n",
       " -0.43916297841186464,\n",
       " -0.43916229910629856,\n",
       " -0.43916161980792046,\n",
       " -0.43916094051673027,\n",
       " -0.4391602612327279,\n",
       " -0.43915958195591365,\n",
       " -0.4391589026862875,\n",
       " -0.4391582234238493,\n",
       " -0.4391575441685993,\n",
       " -0.4391568649205373,\n",
       " -0.43915618567966364,\n",
       " -0.4391555064459781,\n",
       " -0.43915482721948085,\n",
       " -0.4391541480001718,\n",
       " -0.4391534687880511,\n",
       " -0.4391527895831187,\n",
       " -0.4391521103853747,\n",
       " -0.43915143119481925,\n",
       " -0.43915075201145215,\n",
       " -0.4391500728352734,\n",
       " -0.4391493936662834,\n",
       " -0.43914871450448184,\n",
       " -0.43914803534986896,\n",
       " -0.43914735620244455,\n",
       " -0.43914667706220883,\n",
       " -0.4391459979291619,\n",
       " -0.43914531880330365,\n",
       " -0.4391446396846342,\n",
       " -0.43914396057315347,\n",
       " -0.43914328146886156,\n",
       " -0.43914260237175856,\n",
       " -0.4391419232818444,\n",
       " -0.4391412441991192,\n",
       " -0.4391405651235829,\n",
       " -0.43913988605523574,\n",
       " -0.4391392069940775,\n",
       " -0.4391385279401084,\n",
       " -0.4391378488933284,\n",
       " -0.43913716985373735,\n",
       " -0.4391364908213357,\n",
       " -0.43913581179612315,\n",
       " -0.43913513277809996,\n",
       " -0.43913445376726584,\n",
       " -0.4391337747636212,\n",
       " -0.4391330957671658,\n",
       " -0.43913241677789966,\n",
       " -0.4391317377958232,\n",
       " -0.439131058820936,\n",
       " -0.4391303798532384,\n",
       " -0.4391297008927302,\n",
       " -0.43912902193941156,\n",
       " -0.4391283429932825,\n",
       " -0.4391276640543431,\n",
       " -0.43912698512259335,\n",
       " -0.4391263061980331,\n",
       " -0.43912562728066273,\n",
       " -0.4391249483704822,\n",
       " -0.4391242694674912,\n",
       " -0.4391235905716903,\n",
       " -0.4391229116830791,\n",
       " -0.43912223280165774,\n",
       " -0.4391215539274264,\n",
       " -0.43912087506038494,\n",
       " -0.43912019620053344,\n",
       " -0.439119517347872,\n",
       " -0.4391188385024007,\n",
       " -0.4391181596641193,\n",
       " -0.43911748083302826,\n",
       " -0.43911680200912717,\n",
       " -0.43911612319241633,\n",
       " -0.43911544438289585,\n",
       " -0.43911476558056545,\n",
       " -0.4391140867854254,\n",
       " -0.43911340799747567,\n",
       " -0.43911272921671635,\n",
       " -0.4391120504431473,\n",
       " -0.4391113716767689,\n",
       " -0.43911069291758076,\n",
       " -0.43911001416558315,\n",
       " -0.439109335420776,\n",
       " -0.4391086566831595,\n",
       " -0.4391079779527336,\n",
       " -0.4391072992294983,\n",
       " -0.4391066205134538,\n",
       " -0.43910594180459983,\n",
       " -0.43910526310293657,\n",
       " -0.43910458440846417,\n",
       " -0.4391039057211826,\n",
       " -0.4391032270410918,\n",
       " -0.4391025483681919,\n",
       " -0.43910186970248277,\n",
       " -0.4391011910439647,\n",
       " -0.4391005123926376,\n",
       " -0.4390998337485014,\n",
       " -0.4390991551115563,\n",
       " -0.4390984764818023,\n",
       " -0.4390977978592393,\n",
       " -0.4390971192438676,\n",
       " -0.4390964406356869,\n",
       " -0.43909576203469747,\n",
       " -0.43909508344089926,\n",
       " -0.4390944048542923,\n",
       " -0.43909372627487675,\n",
       " -0.4390930477026524,\n",
       " -0.4390923691376194,\n",
       " -0.439091690579778,\n",
       " -0.4390910120291278,\n",
       " -0.4390903334856692,\n",
       " -0.4390896549494021,\n",
       " -0.43908897642032657,\n",
       " -0.4390882978984426,\n",
       " -0.4390876193837502,\n",
       " -0.43908694087624933,\n",
       " -0.4390862623759403,\n",
       " -0.43908558388282287,\n",
       " -0.43908490539689715,\n",
       " -0.4390842269181634,\n",
       " -0.4390835484466213,\n",
       " -0.43908286998227103,\n",
       " -0.4390821915251126,\n",
       " -0.43908151307514615,\n",
       " -0.4390808346323716,\n",
       " -0.439080156196789,\n",
       " -0.4390794777683984,\n",
       " -0.43907879934719996,\n",
       " -0.4390781209331935,\n",
       " -0.43907744252637915,\n",
       " -0.4390767641267569,\n",
       " -0.4390760857343268,\n",
       " -0.439075407349089,\n",
       " -0.4390747289710434,\n",
       " -0.4390740506001901,\n",
       " -0.439073372236529,\n",
       " -0.4390726938800604,\n",
       " -0.439072015530784,\n",
       " -0.4390713371887001,\n",
       " -0.43907065885380864,\n",
       " -0.4390699805261096,\n",
       " -0.4390693022056031,\n",
       " -0.43906862389228907,\n",
       " -0.43906794558616774,\n",
       " -0.4390672672872389,\n",
       " -0.4390665889955027,\n",
       " -0.43906591071095913,\n",
       " -0.43906523243360845,\n",
       " -0.4390645541634503,\n",
       " -0.43906387590048507,\n",
       " -0.43906319764471263,\n",
       " -0.439062519396133,\n",
       " -0.43906184115474617,\n",
       " -0.43906116292055225,\n",
       " -0.43906048469355125,\n",
       " -0.4390598064737433,\n",
       " -0.4390591282611283,\n",
       " -0.4390584500557064,\n",
       " -0.43905777185747735,\n",
       " -0.4390570936664417,\n",
       " -0.43905641548259894,\n",
       " -0.4390557373059495,\n",
       " -0.4390550591364932,\n",
       " -0.43905438097423005,\n",
       " -0.43905370281916023,\n",
       " -0.43905302467128365,\n",
       " -0.4390523465306005,\n",
       " -0.4390516683971107,\n",
       " -0.4390509902708143,\n",
       " -0.4390503121517113,\n",
       " -0.43904963403980163,\n",
       " -0.43904895593508575,\n",
       " -0.4390482778375632,\n",
       " -0.4390475997472341,\n",
       " -0.4390469216640989,\n",
       " -0.43904624358815714,\n",
       " -0.439045565519409,\n",
       " -0.4390448874578545,\n",
       " -0.43904420940349403,\n",
       " -0.43904353135632707,\n",
       " -0.4390428533163539,\n",
       " -0.4390421752835746,\n",
       " -0.43904149725798913,\n",
       " -0.43904081923959765,\n",
       " -0.43904014122839996,\n",
       " -0.43903946322439635,\n",
       " -0.4390387852275866,\n",
       " -0.43903810723797093,\n",
       " -0.4390374292555493,\n",
       " -0.4390367512803217,\n",
       " -0.4390360733122882,\n",
       " -0.4390353953514491,\n",
       " -0.43903471739780386,\n",
       " -0.43903403945135305,\n",
       " -0.43903336151209643,\n",
       " -0.43903268358003406]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxddZ3/8dcn+54mTZomabpSurEUCAiyjEBBQQeQYdVRBsTqKOMuojgzjDMqroz+GBXEBQVBZBEG0LILjGwtW/d9S5c03bI0bdMkn98f56TcxiRN03tz7k3ez8fjPnLv95x7zufktved7/ds5u6IiIgcrrSoCxARkaFBgSIiInGhQBERkbhQoIiISFwoUEREJC4UKCIiEhcKFJEkYmYfNLP1ZtZiZsdFXQ+AmX3NzO6I97wy9JjOQ5Eomdka4Fp3fyrqWpKBma0EvuDuD8dpec8Bd7m7vuQl4dRDETkMZpYR50WOAxYOsJb0Abwn3vXLMKZAkaRkZtlm9t9mtjF8/LeZZYfTyszsUTPbaWbbzewFM0sLp33FzDaYWbOZLTWzs3tZfq6Z/cDM1ppZo5m9GLa9x8zqus27xsxmhc9vMrP7zewuM2sCvmZmu82sNGb+48xsq5llhq+vMbPFZrbDzOaY2bhetrcFSAfeCnsqmNk0M3su3NaFZnZBzHt+bWY/NbPHzWwXcGa3ZX4TOB24NRxCuzVsdzP7tJktB5aHbT8Kh9qazGyemZ0es5ybzOyu8Pn48P1Xmdm6cDtvHOC8uWZ2Z/h7WWxm13f/3UtqUaBIsroROBmYCRwLnAR8PZz2RaAOKAcqgK8BbmZTgOuAE929EHgvsKaX5X8fOAF4N1AKXA909rO2C4H7gRHA94CXgH+Imf4h4H5332dmF4b1XRzW+wJwT/cFuvtedy8IXx7r7pPCQPpf4AlgFPAvwN3hdsau65tAIfBit2XeGK7vOncvcPfrYiZfBLwLmB6+fo3gd10K/A74g5nl9PE7OA2YApwN/JuZTRvAvP8OjAcmAucA/9jHMiQFKFAkWX0Y+Ia7b3H3BuA/gI+E0/YBlcA4d9/n7i94sDOwA8gGpptZpruvcfeV3Rcc9mauAT7r7hvcvcPd/+rue/tZ20vu/kd373T33QRfwFeGyzbgirAN4JPAt919sbu3A98CZvbUS+nByUABcLO7t7n7M8CjXesKPezu/xfWsqef9RPWtD2sH3e/y923uXu7u/+A4Pc4pY/3/4e773b3t4C3CEL/UOe9DPiWu+9w9zrgx4dQvyQhBYokqypgbczrtWEbBL2CFcATZrbKzG4AcPcVwOeAm4AtZnavmVXxt8qAHOBvwqaf1nd7/QBwiplVAmcQ9HReCKeNA34UDlntBLYDBlT3Yz1VwHp3j+05re323u619NcB7zOzL4XDTo1hncUEv6febI553koQfIc6b1W3Oga6LZIkFCiSrDYSfBl3GRu24e7N7v5Fd58IXAB8oWtfibv/zt1PC9/rwHd6WPZWYA8wqYdpu4C8rhfhju7ybvMccGiku+8gGJa6nGAI6l5/5/DJ9cAn3H1EzCPX3f960N9AsL01XfuHQmOBDb3V0oPepu9vD/eXXE/QYyhx9xFAI0HwJdImYEzM65oEr08STIEiySDTzHJiHhkE+xm+bmblZlYG/BvQtbP3A2Z2RDi81Egw1NVpZlPM7Kxw5/0eYDc97BcJ/+L/JfBDM6sys3QzOyV83zIgx8zeH+7D+DrB8M/B/A74KHAJ7wx3AfwM+KqZzQhrLzazS/v5e3mF4C/6680s08zeA/w9cG8/3w9QT7CPoi+FQDvQAGSY2b8BRYewjoG6j+B3U2Jm1QT7vySFKVAkGTxO8OXf9bgJ+C9gLvA2MB94PWwDmAw8BbQQ7BD/ibs/S/DFfzNBD2QzwY7sr/ayzi+Fy32NYBjqO0CauzcCnwLuIOgJ7CI4AOBgHgnr2hzuKwDA3R8Kl31veFTYAuC8fiwPd28jCJDzwm36CfBRd1/Sn/eHfgRcEh5J1ds+ijnAnwnCdC1BGA/G8NM3CH63qwk+z/uB/u7HkiSkExtFJCmY2T8DV7j730VdiwyMeigiEgkzqzSzU80sLTwU+ovAQ1HXJQOns2RFJCpZwG3ABGAnwb6hn0RakRwWDXmJiEhcaMhLRETiYlgNeZWVlfn48eOjLkNEJKXMmzdvq7t3Px/rbwyrQBk/fjxz586NugwRkZRiZmsPPpeGvEREJE4UKCIiEhcKFBERiQsFioiIxIUCRURE4kKBIiIicRFZoJhZqZk9aWbLw58lvczXYWZvho9HYtonmNkrZrbCzH5vZlmDV72IiHQXZQ/lBuBpd58MPB2+7slud58ZPi6Iaf8OcIu7HwHsAD6WqEJfWN7Arc8s5/55dSzY0Ehbe39vPS4iMnxEeWLjhcB7wud3As8BX+nPG8MbK51FcHe8rvffBPw0ngV2eXrxFn791zX7X2emG1NGF3JUVTEzqos5qqqIaZVF5GSmJ2L1IiIpIbKLQ5rZzvBWo10BsaPrdbf52oE3Ce4od7O7/zG8g9/LYe8EM6sB/uTuR/Xw/tnAbICxY8eesHZtv074PMDutg7MYMPO3Sze1MSCDU0s3NjIgg2N7GjdB0BBdgZ/f2wlHzttIkeM6uv22iIiqcXM5rl77cHmS2gPxcyeAkb3MOnG2Bfu7mbWW7KNc/cNZjYReMbM5hPc9rVf3P124HaA2traAaVnblbQ85hUXsCk8gI+cExV17LZ2LiH+XWNPLW4nofe2MB9c+v40EljueG8qeRnD6sr24jIMJfQbzx3n9XbNDOrN7NKd99kZpXAll6WsSH8ucrMngOOAx4ARphZhru3A2MIbtc6qMyM6hG5VI/I5X1HjeaG86by46eX89uX1/Liiq389B+PZ+rowbg1t4hI9KLcKf8IcFX4/Crg4e4zmFmJmWWHz8uAU4FFHozTPQtc0tf7B1tZQTbfuPAo7vn4ybS2tXPpz17i5VXboi5LRGRQRBkoNwPnmNlyYFb4GjOrNbM7wnmmAXPN7C2CALnZ3ReF074CfMHMVgAjgV8MavV9OHniSB781KmMKszmql++yisKFREZBobVHRtra2t9MC9fv61lL5fe9hINTXu5Z/bJHFVdPGjrFhGJl/7ulNeZ8gk0siCbuz72LgpzMrj616+xqXF31CWJiCSMAiXBqkbk8utrTqJ1bzuzfzOP3W0dUZckIpIQCpRBcGRFIT+64jgWbGzky/e/xXAaZhSR4UOBMkhmTa/g+vdO5dG3N/GT51ZGXY6ISNwpUAbRJ/9uIhccW8UPnljKX1dujbocEZG4UqAMIjPj2xcfzfiyfD5775s0NO+NuiQRkbhRoAyy/OwMfvLh42navY8v3Pem9qeIyJChQInA1NFFfP3903hh+VYem78p6nJEROJCgRKRD71rHNMri/jOn5fQ0aleioikPgVKRNLTjOvOOoL123fzzJIer4spIpJSFCgROnd6BeWF2Tz4el3UpYiIHDYFSoQy0tOYNa2C55c1sLddZ9CLSGpToETs7Kmj2NXWwetrd0ZdiojIYVGgROyEcSUAvLF+R8SViIgcHgVKxErys5hYlq8eioikPAVKEphZM4K36xQoIpLaFChJYMroQrY076WxdV/UpYiIDJgCJQlMrigAYPmW5ogrEREZOAVKEpg8qhCA5VtaIq5ERGTgFChJoHpELnlZ6SyrVw9FRFJXJIFiZqVm9qSZLQ9/lvQwz5lm9mbMY4+ZXRRO+7WZrY6ZNnPwtyJ+0tKMCWX5rN66K+pSREQGLKoeyg3A0+4+GXg6fH0Ad3/W3We6+0zgLKAVeCJmli93TXf3Nwel6gQaNzKPddtboy5DRGTAogqUC4E7w+d3AhcdZP5LgD+5+5D9xq0pzaNu+25deVhEUlZUgVLh7l03AtkMVBxk/iuAe7q1fdPM3jazW8wsO+4VDrJxpfm0dXRS37Qn6lJERAYkYYFiZk+Z2YIeHhfGzufBLQt7/bPczCqBo4E5Mc1fBaYCJwKlwFf6eP9sM5trZnMbGhoOZ5MSamxpHgBrtw3ZTpiIDHEZiVqwu8/qbZqZ1ZtZpbtvCgOjrxuCXAY85O77z/qL6d3sNbNfAV/qo47bgdsBamtrk3Y8qStQ1m9v5ZRJIyOuRkTk0EU15PUIcFX4/Crg4T7mvZJuw11hCGFmRrD/ZUECahxUVSNySE8z1m7XkV4ikpqiCpSbgXPMbDkwK3yNmdWa2R1dM5nZeKAG+Eu3999tZvOB+UAZ8F+DUHNCZaSnUT0iV0NeIpKyEjbk1Rd33wac3UP7XODamNdrgOoe5jsrkfVFpaY0l7odu6MuQ0RkQHSmfBKpKcmjbod6KCKSmhQoSaSmNI+tLW20trVHXYqIyCFToCSRMSW5ABr2EpGUpEBJIjUxhw6LiKQaBUoS6eqhKFBEJBUpUJJIeUE2OZlpGvISkZSkQEkiZsaYkjzW60gvEUlBCpQkU1OSy/rt6qGISOpRoCSZmlL1UEQkNSlQkkxNSR7Ne9ppbN138JlFRJKIAiXJ1JSGR3qplyIiKUaBkmTGlOhcFBFJTQqUJFMTBooOHRaRVKNASTLFeZkU5mRoyEtEUo4CJQnVlORpyEtEUo4CJQnVlOayXkNeIpJiFChJqOu+KO4edSkiIv2mQElCNaV57NnXSUPL3qhLERHpNwVKEnrnqsMa9hKR1KFASUJd90XR7YBFJJUoUJKQ7twoIqkoskAxs0vNbKGZdZpZbR/zvc/MlprZCjO7IaZ9gpm9Erb/3syyBqfyxMvLyqCsIEuHDotISomyh7IAuBh4vrcZzCwd+B/gPGA6cKWZTQ8nfwe4xd2PAHYAH0tsuYNL90URkVQTWaC4+2J3X3qQ2U4CVrj7KndvA+4FLjQzA84C7g/nuxO4KHHVDr6a0jztlBeRlJLs+1CqgfUxr+vCtpHATndv79Y+ZNSU5LJx5246OnUuioikhoxELtzMngJG9zDpRnd/OJHrjqlhNjAbYOzYsYOxyrgYU5JHe6ezqXH3/isQi4gks4QGirvPOsxFbABqYl6PCdu2ASPMLCPspXS191TD7cDtALW1tSnz537XfVHqdihQRCQ1JPuQ12vA5PCIrizgCuARD65J8ixwSTjfVcCg9HgGS43uiyIiKSbKw4Y/aGZ1wCnAY2Y2J2yvMrPHAcLex3XAHGAxcJ+7LwwX8RXgC2a2gmCfyi8GexsSqWpELmboIpEikjISOuTVF3d/CHioh/aNwPkxrx8HHu9hvlUER4ENSVkZaVQW5VCnHoqIpIhkH/Ia1saU6lwUEUkdCpQkFtxoS0NeIpIaFChJbExJLvXNe9jb3hF1KSIiB6VASWITy/Nxh7XbNOwlIslPgZLEjhhVAMDy+paIKxEROTgFShKbVF6AGSzf0hx1KSIiB6VASWI5menUlOSxfIt6KCKS/BQoSW7yqAJWaMhLRFKAAiXJHVFRwKqtLbR3dEZdiohInxQoSW7yqEL2dTjrdMa8iCQ5BUqSm9x1pJf2o4hIklOgJLlJYaCsUKCISJJToCS5guwMxpTksnhTU9SliIj0SYGSAqZXFrFoowJFRJKbAiUFzKgqZvW2Xeza2x51KSIivVKgpIAZVUW4w5LN6qWISPJSoKSA6VVFACzUsJeIJDEFSgqoLM6hJC+ThRsUKCKSvA45UMwszcyKElGM9MzMmFFVzMJNjVGXIiLSq34Fipn9zsyKzCwfWAAsMrMvJ7Y0iTWjqohlm1vYp0uwiEiS6m8PZbq7NwEXAX8CJgAfSVhV8jeOqi6mraOTpZt1KXsRSU79DZRMM8skCJRH3H0f4ANdqZldamYLzazTzGp7mafGzJ41s0XhvJ+NmXaTmW0wszfDx/kDrSVVHDd2BACvr9sRcSUiIj3rb6DcBqwB8oHnzWwccDh7iBcAFwPP9zFPO/BFd58OnAx82symx0y/xd1nho/HD6OWlFA9IpfywmzeWLcz6lJERHqU0Z+Z3P3HwI9jmtaa2ZkDXam7L4ZgZ3Mf82wCNoXPm81sMVANLBroelOZmXH82BHqoYhI0urvTvnPhjvlzcx+YWavA2cluLbY9Y8HjgNeiWm+zszeNrNfmllJH++dbWZzzWxuQ0NDgitNrOPHlrB2WytbW/ZGXYqIyN/o75DXNeFO+XOBEoId8jf39QYze8rMFvTwuPBQCjSzAuAB4HNhDQA/BSYBMwl6MT/o7f3ufru717p7bXl5+aGsOukcPy7ITQ17iUgy6teQF9A1NnU+8Ft3X2h9jVcB7j7rsCoDwgMBHgDudvcHY5ZdHzPPz4FHD3ddqeDo6mIy0ozX1+3gnOkVUZcjInKA/vZQ5pnZEwSBMsfMCoGEnhARBtYvgMXu/sNu0ypjXn6QYCf/kJeTmc6MqiLmrdF+FBFJPv0NlI8BNwAnunsrkAVcPdCVmtkHzawOOAV4zMzmhO1VZtZ1xNapBENrZ/VwePB3zWy+mb0NnAl8fqC1pJqTJ47kzfU72d3WEXUpIiIH6O9RXp1mNgb4UDjS9Rd3/9+BrtTdHwIe6qF9I0EvCHd/kXeG2rrPN2xPqjx50khue34V89bu4LTJZVGXIyKyX3+P8roZ+CzBIbuLgM+Y2bcSWZj07MTxpWSkGS+t2hp1KSIiB+jvTvnzgZnu3glgZncCbwBfS1Rh0rOC7AyOGVPMSyu3RV2KiMgBDuVqwyNinhfHuxDpv1MmjeStukZadAdHEUki/Q2UbwNvmNmvw97JPOCbiStL+vLuSWV0dDqvrdkedSkiIvv1K1Dc/R6C62k9SHBeyCnu/vtEFia9O2FcCVkZabywTPtRRCR59BkoZnZ81wOoBOrCR1XYJhHIyUznlIkjeW7plqhLERHZ72A75Xu9pAnB5esH7XpecqCzpo7i3x9ZyJqtuxhflh91OSIifQeKuw/4isKSWGdOGcW/s5Bnl27h6rIJUZcjItK/w4bN7OIemhuB+e6ucZcIjB2Zx6TyfJ5ZsoWrT1WgiEj0+nseyscILpPybPj6PQRHek0ws2+4+28TUJscxFlTR3HnX9fS2tZOXlZ/P0oRkcTo72HDGcA0d/8Hd/8HYDrBPpR3AV9JVHHStzOnjKKto5MXlutoLxGJXn8DpSb2kvHAlrBtO7Av/mVJf5w4oZSSvEz+vGBz1KWIiPR7yOs5M3sU+EP4+pKwLR/Q3Z4ikpmexrnTR/P4/E3sbe8gOyM96pJEZBjrbw/l08CvCO6QOBO4E/i0u+/SkWDROu/o0TTvbedFDXuJSMT6e6a8Ay8CzwBPA8+HbRKxU48oozg3k8fna9hLRKLV38vXXwa8SjDUdRnwipldksjCpH8y09M4Z3oFTy7aTFt7Qm+iKSLSp/4Oed1IcLfGq9z9o8BJwL8mriw5FOcfPZqmPe28uKIh6lJEZBjrb6CkdTuBcdshvFcS7LQjyinJy+ShNzZGXYqIDGP9Pcrrz+F93+8JX18OPN7H/DKIsjLSuODYKu59bT1Ne/ZRlJMZdUkiMgz1d6f8l4HbgWPCx+3urhMak8jFx49hb3snf5q/KepSRGSY6vf1Otz9AYJ7oUgSOmZMMZPK83lg3gYuP3Fs1OWIyDB0sPuhNJtZUw+PZjNrGuhKzexSM1toZp1mVtvHfGvMbL6ZvWlmc2PaS83sSTNbHv4sGWgtQ4WZcfHxY3h1zXbWbWuNuhwRGYb6DBR3L3T3oh4ehe5edBjrXQBcDDzfj3nPdPeZ7h4bPDcAT7v7ZILzYm44jFqGjIuOq8YMHni9LupSRGQYiuRILXdf7O5LD2MRFxKcrU/486LDryr1VY/I5bQjyrhv7nraO3ROiogMrmQ/9NeBJ8xsnpnNjmmvcPeuvc+bgYreFmBms81srpnNbWgY+udpfPhd49jUuIdnlw79bRWR5JKwQDGzp8xsQQ+PCw9hMae5+/HAecCnzeyM7jOEl4Dp9TIw7n67u9e6e215efmhb0iKmTVtFBVF2dz18tqoSxGRYSZhd2Vy91lxWMaG8OcWM3uI4Az954F6M6t0901mVklwOX0BMtLTuOLEsfz4meWs29bK2JF5UZckIsNE0g55mVm+mRV2PQfOJdiZD/AIcFX4/Crg4cGvMHldcVINBtzz2rqoSxGRYSSSQDGzD5pZHcFthR8Lz8LHzKrMrOsM/ArgRTN7i+DClI+5+5/DaTcD55jZcmBW+FpClcW5nD2tgvteW8+efR1RlyMiw0QkNyJ394eAh3po3wicHz5fBRzby/u3AWcnssZUd/Wp43lyUT0PvbGBK0/SiY4iknhJO+Qlh+eUiSOZUVXEHS+sorNTt64RkcRToAxRZsbsMyaysmEXzy3TMQsikngKlCHs/KMrqSzO4fbnV0VdiogMAwqUISwzPY1rTp3Ay6u2M7+uMepyRGSIU6AMcVecVENhTga3Prs86lJEZIhToAxxhTmZfOy0CcxZWM/CjeqliEjiKFCGgatPnUBhTgY/flq9FBFJHAXKMFCcm8k1pwa9lEUbB3wbGxGRPilQholrTgt6KT96elnUpYjIEKVAGSaKczO59rSJzFlYz+vrdkRdjogMQQqUYeTa0ydQXpjNtx5bTHDVfxGR+FGgDCP52Rl8ftaRzF27gzkL66MuR0SGGAXKMHNZ7RiOGFXAd/68hH26TbCIxJECZZjJSE/jhvdNZfXWXbqro4jElQJlGDp72ihOO6KMHz65jC3Ne6IuR0SGCAXKMGRmfOPCGezd18m3H18SdTkiMkQoUIapieUFfOLvJvLQGxt4edW2qMsRkSFAgTKMffrMI6gpzeVf/7iAtnbtoBeRw6NAGcZyMtP5jwtmsHxLCz95bkXU5YhIilOgDHNnTa3goplV3PrMCl2NWEQOiwJFuOmCGZTkZ/HF+97S0JeIDFgkgWJml5rZQjPrNLPaXuaZYmZvxjyazOxz4bSbzGxDzLTzB3cLhpYReVl8+4NHs2RzM7c+q6EvERmYqHooC4CLged7m8Hdl7r7THefCZwAtAIPxcxyS9d0d388seUOfbOmV3DxcdX8z7MrmLdWF48UkUMXSaC4+2J3X3oIbzkbWOnuOrU7gW66cAZVI3L4zD1v0Ni6L+pyRCTFpMo+lCuAe7q1XWdmb5vZL82spLc3mtlsM5trZnMbGhoSW2WKK8rJ5P9deTz1TXu44cG3dUViETkkCQsUM3vKzBb08LjwEJeTBVwA/CGm+afAJGAmsAn4QW/vd/fb3b3W3WvLy8sHsCXDy8yaEVz/vin8acFm7nplXdTliEgKyUjUgt19VpwWdR7wurvvv9567HMz+znwaJzWJcC1p03kryu38Z+PLuKY6mKOrRkRdUkikgJSYcjrSroNd5lZZczLDxLs5Jc4SUszfnjZTEYVZvOJ387TBSRFpF+iOmz4g2ZWB5wCPGZmc8L2KjN7PGa+fOAc4MFui/iumc03s7eBM4HPD1Lpw0Zpfha3f6SWnbvb+NRdr+v8FBE5KBtOO15ra2t97ty5UZeRUv73rY38yz1vcOVJY/n2xUdHXY6IRMDM5rl7j+cMxkrYPhQZGv7+2CoWbmziZ39ZyaTyfK49fWLUJYlIklKgyEF9+b1TWLttF998fDGVxbm8/5jKg79JRIadVNgpLxFLTzNuuXwmJ4wt4fP3vcmrq7dHXZKIJCEFivRLTmY6P/9oLWNKcvn4b+ayvL456pJEJMkoUKTfSvKzuPPqk8jKSOPDd7zCmq27oi5JRJKIAkUOSU1pHndf+y7aO50P3/EKdTtaoy5JRJKEAkUO2ZEVhfzmmpNo3rOPD9/xCpsbdeKjiChQZICOqi7mzmtOYltLGx+642WFiogoUGTgjhtbwq+uPpEtTXu57LaXWL9dw18iw5kCRQ7LieNLufvad9G0Zx+X/uwlVmxpibokEYmIAkUO27E1I7h39sm0dzqX3/YSCzc2Rl2SiERAgSJxMXV0Efd94mSyM9K4/LaXeWG5bmYmMtwoUCRuJpYX8MCn3s2Yklyu/tVr3Dd3fdQlicggUqBIXFUW5/KHT57CKZNGcv39b3PLk8t0K2GRYUKBInFXmJPJL//pRC49YQw/eno5n/v9m+xu64i6LBFJMF1tWBIiMz2N715yDOPL8vn+E0tZXt/CbR85gZrSvKhLi4S7s3tfBx2dTkZaGhnpRma6/p6ToUWBIgljZnz6zCOYXlnEZ+59gwtufZFbP3Q8px5RFnVpCdXZ6byxfifPLKln/oYmltc3s62ljbaOA+96WZCdQUl+JqX52YwqzGZsaR41JbmMHZlHTUkeY0fmkZ2RHtFWiBw63bFRBsXqrbuY/Zu5rGxo4fr3TWX26RNJS7Ooy4qrfR2d/GFuHT9/YRWrt+4iPc2YOrqQKRWFVBTnUJybSUaasa/DaWvvZOfuNnbsamPbrjbqm/awfvtudu97Z2gwPc0YPzKPKaMLmTyqkCMrCpkyuoDxI/PJUO9GBlF/79ioQJFB07K3na/c/zaPzd/EGUeW84NLj6W8MDvqsuLi9XU7+NqD81myuZljxxRz1bvHc/bUCorzMvu9DHdna0sb67a3sn57Kyu2tLCsvpll9c2s3d5K13/V7Iw0plUWcVR1EUdVFXNUdTFHVhSSlaGQkcRQoPRAgRI9d+d3r67jG/+7iMKcTG65/FhOn1wedVkD5u786v/W8M3HF1NRmM2/XzCDc6dXYBbf3tfutg5WNrSwdHMzizc1sWBjIws3NNG8tx2AzHRjyuhCjqoqZkZ1MUdVFTGtsoicTA2ZyeFToPRAgZI8lm5u5rrfvc7yLS184oyJfP6cI1Puy6+9o5OvPTSf++bWcc70Cn5w2bEU5fS/R3K4OjudddtbWbCxkQUbmli4sZH5GxrZ2boPCIbMJo8q4KgwYI4eU8y0yiLysrTrVA5N0geKmX0P+HugDVgJXO3uO3uY733Aj4B04A53vzlsnwDcC4wE5gEfcfe2vtapQEkuu9s6+M/HFvG7V9YxqTyf7196LMeNLYm6rH5pa+/k879/k8fmb+IzZx3B52YdmRT7hNyduh27WbixiQUbGsOwaWRrS/BfI82CE1CPri5mRlURR1cXM72qiMJBDCPZO7UAAA8LSURBVEJJPakQKOcCz7h7u5l9B8Ddv9JtnnRgGXAOUAe8Blzp7ovM7D7gQXe/18x+Brzl7j/ta50KlOT0/LIGbnjgbTY37eHjpyd/b2XPvg4+fffrPL1kCzeeP42PnzEx6pL65O7UN+1lwYagB9PVk6lv2rt/noll+cyoLubocL/MjOpiinMVMhJI+kA5oAizDwKXuPuHu7WfAtzk7u8NX381nHQz0ACMDgPpgPl6o0BJXs179vGtx5dwz6vrmFiWz00XzOCMI5Nv38rutg5m/3YuLyzfyn9edBQfOXlc1CUN2JbmPUFPpq5x/7DZhp27908fW5rH1NHB0WWTKwqYMrqQCWX5OpR5GOpvoCTLYOo1wO97aK8GYi8IVQe8i2CYa6e7t8e0V/e0YDObDcwGGDt2bLzqlTgrzMnk2xcfzflHj+Zf/7iAj/7yVc47ajRf/8B0qkfkRl0eEITJtb95jb+u3MZ3LzmGy2proi7psIwqzGHUlBzOnDJqf9v2XW37h8oWbmhiaX0zTy/ZQkdn8IdnepoxoSyfIysKmDyqkCmjCzliVAFjS/OSulcpgyOhgWJmTwGje5h0o7s/HM5zI9AO3J2IGtz9duB2CHooiViHxM/pk8v58+fO4I4XVnHrsyt4bmkD1511BNecOoHcrOi+sFrb2vnYr+fy8uptfP+SY/mHE8ZEVksileZnccaR5Qf0Dve2d7B66y6Wbm5meX0LS+ubWbSxiT8t2Lz/UGYzqCrOZUJZPuPL8phQVsCEsjzGj8ynpjRPVwUYJhIaKO4+q6/pZvZPwAeAs73nsbcNQOyfgWPCtm3ACDPLCHspXe0yBORkpnPdWZO5cGY1//noIr43Zym/eWkNnz37SC6rHTPoJ/XtbG3j2jvn8vq6Hdxy2UwuOq7HzvCQlZ2RztTRRUwdXXRA++62DlZsaWHV1hZWNexizbZdrNm6i4ff3Ejznvb986WnGTUludSU5jGmJI8xJbnhI4+a0lzKC7Ljfpi1RCPKnfLvA34I/J2793jzDDPLINgpfzZBYLwGfMjdF5rZH4AHYnbKv+3uP+lrndqHkppeWbWN7/x5Ca+v28nEsnw+O2sy7z+6clCCZf32Vq761avUbd/NLZfP5P3HVCZ8nanO3dm+q40123axemsrq7e2sGZrK3U7WqnbsZttuw48GDM7I43qMGDGlORS0y10ygqyFDgRS/qd8ma2Asgm6G0AvOzunzSzKoLDg88P5zsf+G+Cw4Z/6e7fDNsnEhw2XAq8Afyju++lDwqU1OXuPLV4C9+bs4Rl9S2MLc3j46dP4NLamoSN3T+1qJ4v3f8W7vDzj9Zy0oTShKxnuGlta6dux+79AdP1fP324OeO8DyaLjmZaX/Ts4n9OTJfgZNoSR8oUVCgpL7OTueJRfX87C8reXP9Tkrzs7i0dgyX1dYwqbwgLutobN3H955Ywl0vr2NGVRG3fuh4JpTlx2XZcnAte9up29HKhjBs1m8Pg2dn8HNnt8DJzUzvNWzGlORSqsA5bAqUHihQhg5359XV2/nFi6v3H4V04vgSLji2ilnTK6gsPvQjw5r37OOeV9fxs7+sYmdrG1efOoEvv3eKjl5KMs179rFh527qtr/Ty1kf09tp3H1g4ORlBYEztjSPCWX54QED+Uwsz2dUofbf9IcCpQcKlKFpS/MeHnx9A/fNXc+qhl0AHF1dzMkTSzl+bAlHVRdTNSKX9G5nsrs7m5v28Ma6nTy5qJ4nF9XTsredd08aydffP53pVUU9rU6SXNOefft7N/sDZ3sr67a3snrrLva2v3MbgfysdMaX5YcBU8DE/c/zdfWAGAqUHihQhjZ3Z2VDC08u2sIzS+p5q66RtvDLIyPNGF2cQ0F2Bpnpaeza205Dy979RyMV52Zy7vQKPnLKOI4ZMyLKzZAE6ux0NjXtYVVDC6u37mJVwy5Wbw0edTta6Yz5OqwszmFyRSFHjirYf3Ln5IpCCrKT5fS9waNA6YECZXhpa+9k0aYmFm1som5HKxt3BvcbaWvvJD87g5H5WUwKL554dHWxzpUY5va2d7B+eysrG3axYksLy+ubWVbfwsqGlgN6NdUjcplcEYZMTNgM5YtuKlB6oEARkUPVEV7VeVl9M8vrm1m+pWV/0HT1gM1gXGkeU0cHtw2YWlnItNFFjCnJTYqLhh6uVLv0iohIUuq63MyEsnzeO+OdC3+0d3SGQRPcp2ZpfROLNzUzZ9E7VxAoyM5gyuhCpo4uZFplEdMqC5kyumjIDpuphyIiEketbe0sq29h8aYmlmxqYnF4U7TYqwfUlOYybXQRUyuLmBaGzdjSvKTtzaiHIiISgbysDGbWjGBmzTsHd7g7Gxv3BAEThsySTU08tbh+/4EAeVnpHFnxTk9m6ugipowuTKnbCKiHIiISkT37OlhW38ySTc0s3hyGzabmA86lqR6Ru3/YbErYm5lQlj+oB5GohyIikuRyMtM5ZsyIAw5V77oh2uJNTSzZ3MySzU0s3dzM88saaA+7M1npaUwaVcC0MGSmVhYxdXRh5CdqKlBERJKIWXDO1OjiHM6c+s69atraO1nZEBwAsDgMmb+u3MaDb7xzofWSvMywNxMEzNTKIo4cxEOaFSgiIikgKyMt3L9SxEUx9xPc2doW9GQ2BTdEW7ypmfvmrqe1rQN455Dmb198DKdMGpnQGhUoIiIpbEReFidPHMnJE98Ji85OZ/2O1jBogmGzsoKshNeiQBERGWLS0oxxI/MZN/LAc2cSvt5BW5OIiAxpChQREYkLBYqIiMSFAkVEROJCgSIiInGhQBERkbhQoIiISFwoUEREJC6G1dWGzawBWDvAt5cBW+NYTirQNg8P2ubh4XC2eZy7lx9spmEVKIfDzOb25/LNQ4m2eXjQNg8Pg7HNGvISEZG4UKCIiEhcKFD67/aoC4iAtnl40DYPDwnfZu1DERGRuFAPRURE4kKBIiIicaFA6Qcze5+ZLTWzFWZ2Q9T1HAozqzGzZ81skZktNLPPhu2lZvakmS0Pf5aE7WZmPw639W0zOz5mWVeF8y83s6ti2k8ws/nhe35sZjb4W/q3zCzdzN4ws0fD1xPM7JWwzt+bWVbYnh2+XhFOHx+zjK+G7UvN7L0x7Un3b8LMRpjZ/Wa2xMwWm9kpQ/1zNrPPh/+uF5jZPWaWM9Q+ZzP7pZltMbMFMW0J/1x7W0ef3F2PPh5AOrASmAhkAW8B06Ou6xDqrwSOD58XAsuA6cB3gRvC9huA74TPzwf+BBhwMvBK2F4KrAp/loTPS8Jpr4bzWvje86Le7rCuLwC/Ax4NX98HXBE+/xnwz+HzTwE/C59fAfw+fD49/LyzgQnhv4P0ZP03AdwJXBs+zwJGDOXPGagGVgO5MZ/vPw21zxk4AzgeWBDTlvDPtbd19Flr1P8Jkv0BnALMiXn9VeCrUdd1GNvzMHAOsBSoDNsqgaXh89uAK2PmXxpOvxK4Lab9trCtElgS037AfBFu5xjgaeAs4NHwP8tWIKP75wrMAU4Jn2eE81n3z7prvmT8NwEUh1+u1q19yH7OBIGyPvySzAg/5/cOxc8ZGM+BgZLwz7W3dfT10JDXwXX9o+1SF7alnLCLfxzwClDh7pvCSZuBivB5b9vbV3tdD+1R+2/geqAzfD0S2Onu7eHr2Dr3b1s4vTGc/1B/F1GaADQAvwqH+e4ws3yG8Ofs7huA7wPrgE0En9s8hvbn3GUwPtfe1tErBcowYWYFwAPA59y9KXaaB3+CDJnjx83sA8AWd58XdS2DKINgWOSn7n4csItgmGK/Ifg5lwAXEoRpFZAPvC/SoiIwGJ9rf9ehQDm4DUBNzOsxYVvKMLNMgjC5290fDJvrzawynF4JbAnbe9vevtrH9NAepVOBC8xsDXAvwbDXj4ARZpYRzhNb5/5tC6cXA9s49N9FlOqAOnd/JXx9P0HADOXPeRaw2t0b3H0f8CDBZz+UP+cug/G59raOXilQDu41YHJ45EgWwc68RyKuqd/CIzZ+ASx29x/GTHoE6DrS4yqCfStd7R8NjxY5GWgMu71zgHPNrCT8y/BcgvHlTUCTmZ0cruujMcuKhLt/1d3HuPt4gs/rGXf/MPAscEk4W/dt7vpdXBLO72H7FeHRQROAyQQ7MJPu34S7bwbWm9mUsOlsYBFD+HMmGOo62czywpq6tnnIfs4xBuNz7W0dvYtyp1qqPAiOnFhGcMTHjVHXc4i1n0bQVX0beDN8nE8wdvw0sBx4CigN5zfgf8JtnQ/UxizrGmBF+Lg6pr0WWBC+51a67RiOePvfwztHeU0k+KJYAfwByA7bc8LXK8LpE2Pef2O4XUuJOaopGf9NADOBueFn/UeCo3mG9OcM/AewJKzrtwRHag2pzxm4h2Af0T6CnujHBuNz7W0dfT106RUREYkLDXmJiEhcKFBERCQuFCgiIhIXChQREYkLBYqIiMSFAkVkAMysJfw53sw+FOdlf63b67/Gc/kiiaJAETk844FDCpSYs7h7c0CguPu7D7EmkUgoUEQOz83A6Wb2pgX35kg3s++Z2Wvh/Sg+AWBm7zGzF8zsEYKzuTGzP5rZPAvu5zE7bLsZyA2Xd3fY1tUbsnDZC8L7V1wes+zn7J17odzddU8LkcF0sL+URKRvNwBfcvcPAITB0OjuJ5pZNvB/ZvZEOO/xwFHuvjp8fY27bzezXOA1M3vA3W8ws+vcfWYP67qY4Gz4Y4Gy8D3Ph9OOA2YAG4H/I7im1Yvx31yR3qmHIhJf5xJcS+lNgtsEjCS4NhTAqzFhAvAZM3sLeJngwn2T6dtpwD3u3uHu9cBfgBNjll3n7p0El9cZH5etETkE6qGIxJcB/+Lucw5oNHsPwSXlY1/PIrjhU6uZPUdwramB2hvzvAP935YIqIcicniaCW6t3GUO8M/hLQMwsyPDG111VwzsCMNkKsEtWLvs63p/Ny8Al4f7acoJbg37aly2QiQO9FeMyOF5G+gIh65+TXDflfHA6+GO8Qbgoh7e92fgk2a2mOAKty/HTLsdeNvMXvfgsvtdHiK4Le1bBFeQvt7dN4eBJBI5XW1YRETiQkNeIiISFwoUERGJCwWKiIjEhQJFRETiQoEiIiJxoUAREZG4UKCIiEhc/H8jvAQbhiVWfgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = DNN(layers=(2, 4, 1))\n",
    "nn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = nn.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, nn.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(x, y, layers=[3, 3, 1], lr=0.01, max_iter=10, tol=0.001):\n",
    "# #     z = np.copy(x)\n",
    "#     layers.insert(0, z.shape[1])\n",
    "#     w = [np.random.normal(0, 0.2, (layers[i], layers[i + 1])) for i in range(len(layers) - 1)]\n",
    "# #     w = [np.random.normal(0, 0.2, (matrix.shape[1], neurons)] + w\n",
    "#     b = [np.zeros((layers[i], )) for i in range(1, len(layers))]\n",
    "# #     arr = [z]\n",
    "# #     w_upd = []\n",
    "#     mult = lr / z.shape[0]\n",
    "#     loss_history = []\n",
    "#     for i in range(max_iter):\n",
    "#         print(f'epoch {i}')\n",
    "#         arr = [np.copy(x)]\n",
    "#         w_upd = []\n",
    "#         for j in range(len(layers) - 2):\n",
    "#             z = np.dot(w[i], arr[0]) + self.b[i]\n",
    "#             arr.append(z)\n",
    "#             z = sigmoid(z)\n",
    "#             arr.append(z)\n",
    "#         z = np.dot(w[-1], z) + b[-1]\n",
    "#         arr.append(z)\n",
    "#         z = sigmoid(z)\n",
    "#         arr.append(z)\n",
    "#         loss_history.append(loss(y, z))\n",
    "#         delta = z - y\n",
    "#         arr.append(delta)\n",
    "# #         print(delta.shape, z.shape)\n",
    "#         dw = np.dot(delta, z.T)\n",
    "#         w_upd.append(dw)\n",
    "#         db = np.sum(delta, axis=1, keepdims=True)\n",
    "#         w_upd.append(db)\n",
    "#         for j in range(len(layers) - 2, 0, -1):\n",
    "#             delta = np.dot(np.dot(delta, w[j]), sigmoid_derivative(2 * j - 1))\n",
    "#             dw = np.dot(delta, arr[2 * j].T)\n",
    "#             w_upd.append(dw)\n",
    "#             db = np.sum(delta, axis=1, keepdims=True)\n",
    "#             w_upd.append(db)\n",
    "#         for j in range(len(w)):\n",
    "#             w[j] += - mult * w_upd[2 * j]\n",
    "#             b[j] += - mult * w_upd[2 * j + 1]\n",
    "#         if loss[-1] - loss[-2] < tol:\n",
    "#             print(f'Algorithm converged at epoch {i}')\n",
    "#             break\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet(layers=[2, 8, 1])\n",
    "net.fit(X, y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "net.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNet():\n",
    "    '''\n",
    "    A two layer neural network\n",
    "    '''\n",
    "        \n",
    "    def __init__(self, layers=[13,8,1], learning_rate=0.01, iterations=100):\n",
    "        self.params = {}\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.loss = []\n",
    "        self.sample_size = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "                \n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        Initialize the weights from a random normal distribution\n",
    "        '''\n",
    "        np.random.seed(1) # Seed the random number generator\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.params[f'W{i}'] = np.random.randn(self.layers[i - 1], self.layers[i])\n",
    "            self.params[f'b{i}'] = np.random.randn(self.layers[i], )\n",
    "        \n",
    "    def relu(self,Z):\n",
    "        '''\n",
    "        The ReLufunction performs a threshold\n",
    "        operation to each input element where values less \n",
    "        than zero are set to zero.\n",
    "        '''\n",
    "        return np.maximum(0, Z)\n",
    "        \n",
    "        \n",
    "    def sigmoid(self,Z):\n",
    "        '''\n",
    "        The sigmoid function takes in real numbers in any range and \n",
    "        squashes it to a real-valued output between 0 and 1.\n",
    "        '''\n",
    "        return 1.0 / (1.0 + np.exp(-Z))\n",
    "    \n",
    "    def entropy_loss(self,y, yhat):\n",
    "        nsample = len(y)\n",
    "        loss = -1/nsample * (np.sum(np.multiply(np.log(yhat), y) + np.multiply((1 - y), np.log(1 - yhat))))\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def forward_propagation(self, x=None, predict=False):\n",
    "        '''\n",
    "        Performs the forward propagation\n",
    "        '''\n",
    "        Z = self.X if x is None else x\n",
    "        for i in range(1, len(self.layers) - 1):\n",
    "            Z = Z.dot(self.params[f'W{i}']) + self.params[f'b{i}']\n",
    "            self.params[f'Z{i}'] = Z\n",
    "            Z = self.relu(Z)\n",
    "            self.params[f'A{i}'] = Z\n",
    "        k = len(self.layers) - 1\n",
    "        Z = Z.dot(self.params[f'W{k}']) + self.params[f'b{k}']\n",
    "        yhat = self.sigmoid(Z)\n",
    "        if predict:\n",
    "            return yhat\n",
    "        self.params[f'Z{k}'] = Z\n",
    "#         Z1 = self.X.dot(self.params['W1']) + self.params['b1']\n",
    "#         A1 = self.relu(Z1)\n",
    "#         Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "#         yhat = self.sigmoid(Z2)\n",
    "        loss = self.entropy_loss(self.y, yhat)\n",
    "\n",
    "        # save calculated parameters     \n",
    "#         self.params['Z1'] = Z1\n",
    "#         self.params['Z2'] = Z2\n",
    "#         self.params['A1'] = A1\n",
    "\n",
    "        return yhat,loss\n",
    "\n",
    "    \n",
    "    def back_propagation(self,yhat):\n",
    "        '''\n",
    "        Computes the derivatives and update weights and bias according.\n",
    "        '''\n",
    "        def dRelu(x):\n",
    "            z = np.copy(x)\n",
    "#             print('dr', x)\n",
    "            z[z<=0] = 0\n",
    "            z[z>0] = 1\n",
    "            return z\n",
    "         \n",
    "        dl_wrt_yhat = -(np.divide(self.y, yhat) - np.divide((1 - self.y), (1 - yhat)))\n",
    "        dl_wrt_sig = yhat * (1 - yhat)\n",
    "        dl_wrt_z2 = dl_wrt_yhat * dl_wrt_sig\n",
    "        dz = dl_wrt_z2\n",
    "        w_upd = []\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            dw = self.params[f'A{i}'].T.dot(dz)\n",
    "            db = np.sum(dz, axis=0)\n",
    "            w_upd.append(dw)\n",
    "            w_upd.append(db)\n",
    "            dz = dz.dot(self.params[f'W{i + 1}'].T) * dRelu(self.params[f'Z{i}'])\n",
    "#         self.params[f'W2'] = self.params[f'W2'] - self.learning_rate * dw\n",
    "#         self.params[f'b2'] = self.params[f'b2'] - self.learning_rate * db\n",
    "        dw = self.X.T.dot(dz)\n",
    "        db = np.sum(dz, axis=0)\n",
    "        w_upd.append(dw)\n",
    "        w_upd.append(db)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.params[f'W{i}'] += - self.learning_rate * w_upd[- (2 * i)]\n",
    "            self.params[f'b{i}'] += - self.learning_rate * w_upd[- (2 * i) + 1]\n",
    "        \n",
    "#         self.params['W1'] = self.params['W1'] - self.learning_rate * dw\n",
    "#         self.params['b1'] = self.params['b1'] - self.learning_rate * db\n",
    "#         dl_wrt_A1 = dl_wrt_z2.dot(self.params['W2'].T)\n",
    "#         dl_wrt_w2 = self.params['A1'].T.dot(dl_wrt_z2)\n",
    "#         dl_wrt_b2 = np.sum(dl_wrt_z2, axis=0)\n",
    "\n",
    "# #         dl_wrt_z1 = dl_wrt_A1 * dRelu(self.params['Z1'])\n",
    "#         dl_wrt_z1 = dl_wrt_z2.dot(self.params['W2'].T) * dRelu(self.params['Z1'])\n",
    "#         dl_wrt_w1 = self.X.T.dot(dl_wrt_z1)\n",
    "#         dl_wrt_b1 = np.sum(dl_wrt_z1, axis=0)\n",
    "\n",
    "# #         update the weights and bias\n",
    "#         self.params['W1'] = self.params['W1'] - self.learning_rate * dl_wrt_w1\n",
    "#         self.params['W2'] = self.params['W2'] - self.learning_rate * dl_wrt_w2\n",
    "#         self.params['b1'] = self.params['b1'] - self.learning_rate * dl_wrt_b1\n",
    "#         self.params['b2'] = self.params['b2'] - self.learning_rate * dl_wrt_b2\n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the neural network using the specified data and labels\n",
    "        '''\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.init_weights() #initialize weights and bias\n",
    "\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            yhat, loss = self.forward_propagation()\n",
    "            self.back_propagation(yhat)\n",
    "            self.loss.append(loss)\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        return np.round(self.forward_propagation(X, predict=True))\n",
    "#         '''\n",
    "#         Predicts on a test data\n",
    "#         '''\n",
    "#         Z1 = X.dot(self.params['W1']) + self.params['b1']\n",
    "#         A1 = self.relu(Z1)\n",
    "#         Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "#         pred = self.sigmoid(Z2)\n",
    "#         return np.round(pred)              \n",
    "\n",
    "                                \n",
    "    def acc(self, y, yhat):\n",
    "        '''\n",
    "        Calculates the accutacy between the predicted valuea and the truth labels\n",
    "        '''\n",
    "        acc = int(sum(y == yhat) / len(y) * 100)\n",
    "        return acc\n",
    "\n",
    "\n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        Plots the loss curve\n",
    "        '''\n",
    "        plt.plot(self.loss)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"logloss\")\n",
    "        plt.title(\"Loss curve for training\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNet():\n",
    "    '''\n",
    "    A two layer neural network\n",
    "    '''\n",
    "        \n",
    "    def __init__(self, layers=[13,8,1], learning_rate=0.001, iterations=100):\n",
    "        self.params = {}\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.loss = []\n",
    "        self.sample_size = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "                \n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        Initialize the weights from a random normal distribution\n",
    "        '''\n",
    "        np.random.seed(1) # Seed the random number generator\n",
    "        self.params[\"W1\"] = np.random.randn(self.layers[0], self.layers[1]) \n",
    "        self.params['b1']  =np.random.randn(self.layers[1],)\n",
    "        self.params['W2'] = np.random.randn(self.layers[1],self.layers[2]) \n",
    "        self.params['b2'] = np.random.randn(self.layers[2],)\n",
    "        \n",
    "    def relu(self,Z):\n",
    "        '''\n",
    "        The ReLufunction performs a threshold\n",
    "        operation to each input element where values less \n",
    "        than zero are set to zero.\n",
    "        '''\n",
    "        return np.maximum(0, Z)\n",
    "        \n",
    "        \n",
    "    def sigmoid(self,Z):\n",
    "        '''\n",
    "        The sigmoid function takes in real numbers in any range and \n",
    "        squashes it to a real-valued output between 0 and 1.\n",
    "        '''\n",
    "        return 1.0 / (1.0 + np.exp(-Z))\n",
    "    \n",
    "    def entropy_loss(self,y, yhat):\n",
    "        nsample = len(y)\n",
    "        loss = -1/nsample * (np.sum(np.multiply(np.log(yhat), y) + np.multiply((1 - y), np.log(1 - yhat))))\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def forward_propagation(self):\n",
    "        '''\n",
    "        Performs the forward propagation\n",
    "        '''\n",
    "        \n",
    "        Z1 = self.X.dot(self.params['W1']) + self.params['b1']\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "        yhat = self.sigmoid(Z2)\n",
    "        loss = self.entropy_loss(self.y,yhat)\n",
    "\n",
    "        # save calculated parameters     \n",
    "        self.params['Z1'] = Z1\n",
    "        self.params['Z2'] = Z2\n",
    "        self.params['A1'] = A1\n",
    "\n",
    "        return yhat,loss\n",
    "\n",
    "    \n",
    "    def back_propagation(self,yhat):\n",
    "        '''\n",
    "        Computes the derivatives and update weights and bias according.\n",
    "        '''\n",
    "        def dRelu(x):\n",
    "            x[x<=0] = 0\n",
    "            x[x>0] = 1\n",
    "            return x\n",
    "        \n",
    "        dl_wrt_yhat = -(np.divide(self.y,yhat) - np.divide((1 - self.y),(1-yhat)))\n",
    "        dl_wrt_sig = yhat * (1-yhat)\n",
    "        dl_wrt_z2 = dl_wrt_yhat * dl_wrt_sig\n",
    "\n",
    "        dl_wrt_A1 = dl_wrt_z2.dot(self.params['W2'].T)\n",
    "        dl_wrt_w2 = self.params['A1'].T.dot(dl_wrt_z2)\n",
    "        dl_wrt_b2 = np.sum(dl_wrt_z2, axis=0)\n",
    "\n",
    "        dl_wrt_z1 = dl_wrt_A1 * dRelu(self.params['Z1'])\n",
    "        dl_wrt_w1 = self.X.T.dot(dl_wrt_z1)\n",
    "        dl_wrt_b1 = np.sum(dl_wrt_z1, axis=0)\n",
    "\n",
    "        #update the weights and bias\n",
    "        self.params['W1'] = self.params['W1'] - self.learning_rate * dl_wrt_w1\n",
    "        self.params['W2'] = self.params['W2'] - self.learning_rate * dl_wrt_w2\n",
    "        self.params['b1'] = self.params['b1'] - self.learning_rate * dl_wrt_b1\n",
    "        self.params['b2'] = self.params['b2'] - self.learning_rate * dl_wrt_b2\n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the neural network using the specified data and labels\n",
    "        '''\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.init_weights() #initialize weights and bias\n",
    "\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            yhat, loss = self.forward_propagation()\n",
    "            self.back_propagation(yhat)\n",
    "            self.loss.append(loss)\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts on a test data\n",
    "        '''\n",
    "        Z1 = X.dot(self.params['W1']) + self.params['b1']\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "        pred = self.sigmoid(Z2)\n",
    "        return np.round(pred)              \n",
    "\n",
    "                                \n",
    "    def acc(self, y, yhat):\n",
    "        '''\n",
    "        Calculates the accutacy between the predicted valuea and the truth labels\n",
    "        '''\n",
    "        acc = int(sum(y == yhat) / len(y) * 100)\n",
    "        return acc\n",
    "\n",
    "\n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        Plots the loss curve\n",
    "        '''\n",
    "        plt.plot(self.loss)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"logloss\")\n",
    "        plt.title(\"Loss curve for training\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers =  ['age', 'sex','chest_pain','resting_blood_pressure',  \n",
    "        'serum_cholestoral', 'fasting_blood_sugar', 'resting_ecg_results',\n",
    "        'max_heart_rate_achieved', 'exercise_induced_angina', 'oldpeak',\"slope of the peak\",\n",
    "        'num_of_major_vessels','thal', 'heart_disease']\n",
    "\n",
    "heart_df = pd.read_csv('heart.dat', sep=' ', names=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_df.drop(columns=['heart_disease'])\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(1, 0)\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(2, 1)\n",
    "y_label = heart_df['heart_disease'].values.reshape(X.shape[0], 1)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_label, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(Xtrain)\n",
    "Xtrain = sc.transform(Xtrain)\n",
    "Xtest = sc.transform(Xtest)\n",
    "print(f\"Shape of train set is {Xtrain.shape}\")\n",
    "print(f\"Shape of test set is {Xtest.shape}\")\n",
    "print(f\"Shape of train label is {ytrain.shape}\")\n",
    "print(f\"Shape of test labels is {ytest.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNet(layers=[13, 10, 1]) # create the NN model\n",
    "nn.fit(Xtrain, ytrain) #train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNet(layers=[13,10,1], learning_rate=0.01, iterations=500) # create the NN model\n",
    "nn.fit(Xtrain, ytrain) #train the model\n",
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_pred = nn.predict(Xtrain)\n",
    "test_pred = nn.predict(Xtest)\n",
    "\n",
    "print(\"Train accuracy is {}\".format(nn.acc(ytrain, train_pred)))\n",
    "print(\"Test accuracy is {}\".format(nn.acc(ytest, test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X = (hours studying, hours sleeping), y = score on test\n",
    "xAll = np.array(([2, 9], [1, 5], [3, 6], [5, 10]), dtype=float) # input data\n",
    "y = np.array(([92], [86], [89]), dtype=float) # output\n",
    "\n",
    "# scale units\n",
    "xAll = xAll/np.amax(xAll, axis=0) # scaling input data\n",
    "y = y/100 # scaling output data (max test score is 100)\n",
    "\n",
    "# split data\n",
    "X = np.split(xAll, [3])[0] # training data\n",
    "xPredicted = np.split(xAll, [3])[1] # testing data\n",
    "\n",
    "class Neural_Network(object):\n",
    "  def __init__(self):\n",
    "  #parameters\n",
    "    self.inputSize = 2\n",
    "    self.outputSize = 1\n",
    "    self.hiddenSize = 20\n",
    "\n",
    "  #weights\n",
    "    self.W1 = np.random.randn(self.inputSize, self.hiddenSize) # (3x2) weight matrix from input to hidden layer\n",
    "    self.W2 = np.random.randn(self.hiddenSize, self.outputSize) # (3x1) weight matrix from hidden to output layer\n",
    "\n",
    "  def forward(self, X):\n",
    "    #forward propagation through our network\n",
    "    self.z = np.dot(X, self.W1) # dot product of X (input) and first set of 3x2 weights\n",
    "    self.z2 = self.sigmoid(self.z) # activation function\n",
    "    self.z3 = np.dot(self.z2, self.W2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "    o = self.sigmoid(self.z3) # final activation function\n",
    "    return o\n",
    "\n",
    "  def sigmoid(self, s):\n",
    "    # activation function\n",
    "    return 1/(1+np.exp(-s))\n",
    "\n",
    "  def sigmoidPrime(self, s):\n",
    "    #derivative of sigmoid\n",
    "    return s * (1 - s)\n",
    "\n",
    "  def backward(self, X, y, o):\n",
    "    # backward propagate through the network\n",
    "    self.o_error = y - o # error in output\n",
    "    self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
    "\n",
    "    self.z2_error = self.o_delta.dot(self.W2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
    "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
    "\n",
    "    self.W1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
    "    self.W2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
    "\n",
    "  def train(self, X, y):\n",
    "    o = self.forward(X)\n",
    "    self.backward(X, y, o)\n",
    "\n",
    "  def saveWeights(self):\n",
    "    np.savetxt(\"w1.txt\", self.W1, fmt=\"%s\")\n",
    "    np.savetxt(\"w2.txt\", self.W2, fmt=\"%s\")\n",
    "\n",
    "  def predict(self):\n",
    "    print(\"Predicted data based on trained weights: \")\n",
    "    print(\"Input (scaled): \\n\" + str(xPredicted))\n",
    "    print(\"Output: \\n\" + str(self.forward(xPredicted)))\n",
    "\n",
    "NN = Neural_Network()\n",
    "for i in range(1000): # trains the NN 1,000 times\n",
    "  print(\"# \" + str(i) + \"\\n\")\n",
    "  print(\"Input (scaled): \\n\" + str(X))\n",
    "  print(\"Actual Output: \\n\" + str(y))\n",
    "  print(\"Predicted Output: \\n\" + str(NN.forward(X)))\n",
    "  print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss\n",
    "  print(\"\\n\")\n",
    "  NN.train(X, y)\n",
    "\n",
    "NN.saveWeights()\n",
    "NN.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
